{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diminutive Suffix Productivity: corpus processing and cleaning\n",
    "Juan Berrios | jeb358@pitt.edu | Last updated: April 14, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary and overview of the data:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The purpose of the code included in this notebook is to build `DataFrame` objects from the `.txt` files in the corpus directories. I also do some preliminary data cleaning. The corpus I am using is the [*Corpus del español*](https://www.corpusdelespanol.org/); more specifically the [Web/Dialects](https://www.corpusdelespanol.org/web-dial/) corpus. While the corpus is searchable online, it is also possible to access the full data set for those wishing to do computational analyses, such as this. It is necessary to purchase a license to do so. I am authorized to use it through the license of the [Department of Linguistics](https://www.linguistics.pitt.edu/). Samples for the different formats can be downloaded from the [official website](https://www.corpusdata.org/formats.asp). I have also uploaded a copy of the free sample in the [data samples  directory](https://github.com/Data-Science-for-Linguists-2020/Diminutive-Suffix-Productivity/tree/master/data_samples) of this repository. The data set is available in three formats: (i) Database (Structured Query Language), (ii) Word/lemma/PoS, and (iii) linear (raw) text. All are `.txt` files and the former two are tab-delimited. I have chosen to work with the second format because the tags will come in handy and because it's quite compatible with Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "1. [Preparation](#1.-Preparation)  includes the necessary preparations.\n",
    "2. [Loading files](#2.-Loading-files)  includes code for loading the files, cleaning them and turning them into data frames using one of the `.txt` file as a sample.\n",
    "3. [Processing corpus directories](#3.-Processing-corpus-directories)  includes code for performing the operations on a corpus directory containing all the text files of one variety. The resulting data frames are stored as `.pkl` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading libraries and additional settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import glob, pickle, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Turning pretty print off:\n",
    "%pprint\n",
    "\n",
    "#Releasing all output:                                            \n",
    "from IPython.core.interactiveshell import InteractiveShell #Prints all commands rather than the last one.\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `txt` files are very large. For testing purposes, I'll use only one of them as a start. The files are also tab-delimited, which makes my job a little easier. The columns correspond to an ID for the source text, an ID for the token, the token (word), the lemma, and the POS. I will hence use those for column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../../Diminutive-Suffix-Productivity/private/data/wlp_ES-sbo/es-b-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['SourceID', 'TokenID', 'Word', 'Lemma', 'POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First row is ignored because it corresponds to an identifier for the .txt file.\n",
    "\n",
    "df = pd.read_csv(fname,sep='\\t',encoding ='iso-8859-1',skiprows=[0],header=None,names=cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna() #Removing NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27310829, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #It's a very large file (27345213 rows). It will get much smaller once I start cleaning up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403096</td>\n",
       "      <td>Este</td>\n",
       "      <td>este</td>\n",
       "      <td>dd-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403097</td>\n",
       "      <td>es</td>\n",
       "      <td>ser</td>\n",
       "      <td>vip-3s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403098</td>\n",
       "      <td>un</td>\n",
       "      <td>un</td>\n",
       "      <td>li-ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403099</td>\n",
       "      <td>blog</td>\n",
       "      <td>blog</td>\n",
       "      <td>nms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403100</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SourceID     TokenID  Word Lemma      POS\n",
       "0    431270  2206403096  Este  este      dd-\n",
       "1    431270  2206403097    es   ser   vip-3s\n",
       "2    431270  2206403098    un    un    li-ms\n",
       "3    431270  2206403099  blog  blog  nms    \n",
       "4    431270  2206403100    de    de        e"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)   #The lemma column will be useful when I need to aggregations that put lowecase and uppercase \n",
    "            #as well as plural and singular forms together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>27345208</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862343</td>\n",
       "      <td>mas</td>\n",
       "      <td>mas</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345209</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862344</td>\n",
       "      <td>informacion</td>\n",
       "      <td>información</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345210</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862345</td>\n",
       "      <td>visite</td>\n",
       "      <td>visitar</td>\n",
       "      <td>vsp-1/3s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345211</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862346</td>\n",
       "      <td>www.DineroAbundancia.com</td>\n",
       "      <td>www.dineroabundancia.com</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345212</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862347</td>\n",
       "      <td>.</td>\n",
       "      <td>$.</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SourceID     TokenID                      Word  \\\n",
       "27345208    676060  2511862343                       mas   \n",
       "27345209    676060  2511862344               informacion   \n",
       "27345210    676060  2511862345                    visite   \n",
       "27345211    676060  2511862346  www.DineroAbundancia.com   \n",
       "27345212    676060  2511862347                         .   \n",
       "\n",
       "                             Lemma       POS  \n",
       "27345208                       mas        cc  \n",
       "27345209               información         n  \n",
       "27345210                   visitar  vsp-1/3s  \n",
       "27345211  www.dineroabundancia.com         n  \n",
       "27345212                        $.         y  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20684906</td>\n",
       "      <td>611100</td>\n",
       "      <td>1803563509</td>\n",
       "      <td>te</td>\n",
       "      <td>tu</td>\n",
       "      <td>po</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3228921</td>\n",
       "      <td>466920</td>\n",
       "      <td>1589481691</td>\n",
       "      <td>pero</td>\n",
       "      <td>pero</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800789</td>\n",
       "      <td>586970</td>\n",
       "      <td>1715751836</td>\n",
       "      <td>el</td>\n",
       "      <td>el</td>\n",
       "      <td>ld-ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29473</td>\n",
       "      <td>431880</td>\n",
       "      <td>1473416153</td>\n",
       "      <td>esto</td>\n",
       "      <td>esto</td>\n",
       "      <td>pd-3cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15419231</td>\n",
       "      <td>579140</td>\n",
       "      <td>1912395913</td>\n",
       "      <td>lo</td>\n",
       "      <td>lo</td>\n",
       "      <td>po</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SourceID     TokenID  Word Lemma     POS\n",
       "20684906    611100  1803563509    te    tu      po\n",
       "3228921     466920  1589481691  pero  pero      cc\n",
       "16800789    586970  1715751836    el    el   ld-ms\n",
       "29473       431880  1473416153  esto  esto  pd-3cs\n",
       "15419231    579140  1912395913    lo    lo      po"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5) #Everything seems to be loaded correctly as of now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dd-', 'vip-3s', 'li-ms', 'nms    ', 'e', 'nfp    ', 'y',\n",
       "       'nfs    ', 'nmp    ', 'vsp-1/3s', 'n', 'cc', 'o', 'r', 'vps-ms',\n",
       "       'ld-fs', 'm$', 'vc-1/3s', 'vr', 'po', 'li-fs', 'ld-mp', 'vip-3p',\n",
       "       'ld-ms', 'jms    ', 'vpp', 'mc', 'vps-fs', 'j', 'vii-1/3s', 'e_21',\n",
       "       'x', 'cs', 'ld', 'v', 'cc-', 'ld-fp', 'pi-0cn', 'dd', 'dxmp-ind-',\n",
       "       'vip-2s', 'dp-', 'vsp-3p', 'pi-0ms', 'jfp    ', 'cS_21', 'cS_22',\n",
       "       'jmp    ', 'vsp-2s', 'vip-1s', 'vsp-1p', 'ps', 'vip-1p/vis-1p',\n",
       "       'vip-1p', 'pd-3cs', 'vif-1p', 'vps-mp', 'vif-3s', 'vif-3p',\n",
       "       'jfs    ', 'dxfs-ind-', 'vsj-1/3s', 'i', 'pi-3ms', 'vis-3s', 'b',\n",
       "       'p', 'np', 'pr-3cn\"', 'px', 'dxms-ind-', 'pi-3cs', 'dxfs-',\n",
       "       'pr-3cs', 'px-ms', 'dxfp-ind-', 'vis-3p', 'li-mp', 'pi', 'vsi-3p',\n",
       "       'px-mp', 'vsi-1/3s', 'pq-3cn\"', 'vif-2s', 'vif-2p', 'vsp-2p',\n",
       "       'vip-2p', 'vpp-00', 'vm-2p', 'vis-1p', 'dxcs-ind-', 'pr-3cp',\n",
       "       'dxcs-dem-', 'vif-1s', 'vc-1p', 'cC_21', 'cC_22', 'vps-fp',\n",
       "       'vii-3p', 'pq-3cn', 'e_32', 'vis-1s', 'pr-0mp', 'pr-3fs', 'vc-3p',\n",
       "       'vii-1p', 'pv', 'nj', 'fn', 'vc-2p', 'vsi-2s', 'li-fp', 'pp-1cs',\n",
       "       'dxfp-', 'vsi-1p', 'vii-2s', 'vis-2s', 'pr-3ms', 'e_22', 'mo-ms-',\n",
       "       'jn', 'pr-3mp', 'cS_31', 'cS_32', 'cS_33', 'pq-3cp', 'dxfp-int-',\n",
       "       'vii-2p', 'vsj-3p', 'pp-2cs', 'vm-2s', 'pp-2cp', 'fj', 'mo',\n",
       "       'pr-3fs\"', 'pq-3cs', 'f', 'dxcp-dem-', 'e_43', 'vm-3p', 'fv',\n",
       "       'vc-2s', 'pq-3ms', 'fp', 'pron', 'prep', 'mo-fs-', 'pr-3fp',\n",
       "       'vis-2p', 'vm-3s', 'vsf-1/3s', 'e_31', 'vsi-2p', 'mo-mp-', 'N',\n",
       "       'pd-3ms', 'vsj-1p', 'xp', 'dxmp-int-', 'xs', 'xx', 'vsj-2s', 'fx',\n",
       "       'cS_41', 'cS_42', 'cS_43', 'cS_44', 'vsf-3p', 'e_33', 'px-00',\n",
       "       'vsj-2p', 'e_42', 'fnp', 'x_sp', 'pp-2p', 'fnj', 'xe', 'dxfs-int-',\n",
       "       'vsf-2s', 'dxcp-ind-', 'pd-3fs', 'pd-3fp', 'pd-3mp', 'PN_MISC',\n",
       "       'v51    ', 'li', 'de', 'mo-fp-', 'fe', 'xb', 'v29    ', 'v30    ',\n",
       "       'vsf-2p', 'dp', 'ADJ', 'pn', 'xa', 'xy', 'xw', 'm', 'xh', 'xd',\n",
       "       'vsf-1p', 'vsp-', 'a', 'xm'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['POS'].unique()  #These POS tags are not very transparent, but it's a good start. These will also come \n",
    "                    #handy for data clean up because diminutivization applies only to some classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Variety'] = 'ES' #Time to add a column for the variety of Spanish. In this case Spain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403096</td>\n",
       "      <td>Este</td>\n",
       "      <td>este</td>\n",
       "      <td>dd-</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403097</td>\n",
       "      <td>es</td>\n",
       "      <td>ser</td>\n",
       "      <td>vip-3s</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403098</td>\n",
       "      <td>un</td>\n",
       "      <td>un</td>\n",
       "      <td>li-ms</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403099</td>\n",
       "      <td>blog</td>\n",
       "      <td>blog</td>\n",
       "      <td>nms</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403100</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SourceID     TokenID  Word Lemma      POS Variety\n",
       "0    431270  2206403096  Este  este      dd-      ES\n",
       "1    431270  2206403097    es   ser   vip-3s      ES\n",
       "2    431270  2206403098    un    un    li-ms      ES\n",
       "3    431270  2206403099  blog  blog  nms          ES\n",
       "4    431270  2206403100    de    de        e      ES"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #It works out well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>27345208</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862343</td>\n",
       "      <td>mas</td>\n",
       "      <td>mas</td>\n",
       "      <td>cc</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345209</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862344</td>\n",
       "      <td>informacion</td>\n",
       "      <td>información</td>\n",
       "      <td>n</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345210</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862345</td>\n",
       "      <td>visite</td>\n",
       "      <td>visitar</td>\n",
       "      <td>vsp-1/3s</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345211</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862346</td>\n",
       "      <td>www.DineroAbundancia.com</td>\n",
       "      <td>www.dineroabundancia.com</td>\n",
       "      <td>n</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345212</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862347</td>\n",
       "      <td>.</td>\n",
       "      <td>$.</td>\n",
       "      <td>y</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SourceID     TokenID                      Word  \\\n",
       "27345208    676060  2511862343                       mas   \n",
       "27345209    676060  2511862344               informacion   \n",
       "27345210    676060  2511862345                    visite   \n",
       "27345211    676060  2511862346  www.DineroAbundancia.com   \n",
       "27345212    676060  2511862347                         .   \n",
       "\n",
       "                             Lemma       POS Variety  \n",
       "27345208                       mas        cc      ES  \n",
       "27345209               información         n      ES  \n",
       "27345210                   visitar  vsp-1/3s      ES  \n",
       "27345211  www.dineroabundancia.com         n      ES  \n",
       "27345212                        $.         y      ES  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>13969830</td>\n",
       "      <td>566880</td>\n",
       "      <td>1491338568</td>\n",
       "      <td>las</td>\n",
       "      <td>la</td>\n",
       "      <td>ld-fp</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11132087</td>\n",
       "      <td>542350</td>\n",
       "      <td>67032467</td>\n",
       "      <td>¡</td>\n",
       "      <td>$¡</td>\n",
       "      <td>y</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7869575</td>\n",
       "      <td>512780</td>\n",
       "      <td>2505104061</td>\n",
       "      <td>,</td>\n",
       "      <td>$,</td>\n",
       "      <td>y</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5285951</td>\n",
       "      <td>488550</td>\n",
       "      <td>597595099</td>\n",
       "      <td>todos</td>\n",
       "      <td>todo</td>\n",
       "      <td>dxmp-ind-</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8847542</td>\n",
       "      <td>523950</td>\n",
       "      <td>1671410917</td>\n",
       "      <td>sabemos</td>\n",
       "      <td>saber</td>\n",
       "      <td>vip-1p</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SourceID     TokenID     Word  Lemma        POS Variety\n",
       "13969830    566880  1491338568      las     la      ld-fp      ES\n",
       "11132087    542350    67032467        ¡     $¡          y      ES\n",
       "7869575     512780  2505104061        ,     $,          y      ES\n",
       "5285951     488550   597595099    todos   todo  dxmp-ind-      ES\n",
       "8847542     523950  1671410917  sabemos  saber     vip-1p      ES"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A first step involved in cleaning up the data is to remove rows that are not necessary for this analysis. There are two main things to tackle first: symbols and '@' that are stand-ins for words that were removed from the corpus for copyright reasons when it was created. For the former, I can make use of the POS column. Symbols are tagged 'y'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['POS'] != 'y'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403096</td>\n",
       "      <td>Este</td>\n",
       "      <td>este</td>\n",
       "      <td>dd-</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403097</td>\n",
       "      <td>es</td>\n",
       "      <td>ser</td>\n",
       "      <td>vip-3s</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403098</td>\n",
       "      <td>un</td>\n",
       "      <td>un</td>\n",
       "      <td>li-ms</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403099</td>\n",
       "      <td>blog</td>\n",
       "      <td>blog</td>\n",
       "      <td>nms</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403100</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345207</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862342</td>\n",
       "      <td>obtener</td>\n",
       "      <td>obtener</td>\n",
       "      <td>vr</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345208</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862343</td>\n",
       "      <td>mas</td>\n",
       "      <td>mas</td>\n",
       "      <td>cc</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345209</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862344</td>\n",
       "      <td>informacion</td>\n",
       "      <td>información</td>\n",
       "      <td>n</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345210</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862345</td>\n",
       "      <td>visite</td>\n",
       "      <td>visitar</td>\n",
       "      <td>vsp-1/3s</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345211</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862346</td>\n",
       "      <td>www.DineroAbundancia.com</td>\n",
       "      <td>www.dineroabundancia.com</td>\n",
       "      <td>n</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24184571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SourceID     TokenID                      Word  \\\n",
       "0           431270  2206403096                      Este   \n",
       "1           431270  2206403097                        es   \n",
       "2           431270  2206403098                        un   \n",
       "3           431270  2206403099                      blog   \n",
       "4           431270  2206403100                        de   \n",
       "...            ...         ...                       ...   \n",
       "27345207    676060  2511862342                   obtener   \n",
       "27345208    676060  2511862343                       mas   \n",
       "27345209    676060  2511862344               informacion   \n",
       "27345210    676060  2511862345                    visite   \n",
       "27345211    676060  2511862346  www.DineroAbundancia.com   \n",
       "\n",
       "                             Lemma       POS Variety  \n",
       "0                             este       dd-      ES  \n",
       "1                              ser    vip-3s      ES  \n",
       "2                               un     li-ms      ES  \n",
       "3                             blog   nms          ES  \n",
       "4                               de         e      ES  \n",
       "...                            ...       ...     ...  \n",
       "27345207                   obtener        vr      ES  \n",
       "27345208                       mas        cc      ES  \n",
       "27345209               información         n      ES  \n",
       "27345210                   visitar  vsp-1/3s      ES  \n",
       "27345211  www.dineroabundancia.com         n      ES  \n",
       "\n",
       "[24184571 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df #It works. Looks like around 3,000,000 rows were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since '@' is meant to replace words and not symbols, it is tagged as a noun, so the same strategy doesn't work. An alternative is to use the Word column instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Word'] != '@'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403096</td>\n",
       "      <td>Este</td>\n",
       "      <td>este</td>\n",
       "      <td>dd-</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403097</td>\n",
       "      <td>es</td>\n",
       "      <td>ser</td>\n",
       "      <td>vip-3s</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403098</td>\n",
       "      <td>un</td>\n",
       "      <td>un</td>\n",
       "      <td>li-ms</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403099</td>\n",
       "      <td>blog</td>\n",
       "      <td>blog</td>\n",
       "      <td>nms</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403100</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345207</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862342</td>\n",
       "      <td>obtener</td>\n",
       "      <td>obtener</td>\n",
       "      <td>vr</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345208</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862343</td>\n",
       "      <td>mas</td>\n",
       "      <td>mas</td>\n",
       "      <td>cc</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345209</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862344</td>\n",
       "      <td>informacion</td>\n",
       "      <td>información</td>\n",
       "      <td>n</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345210</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862345</td>\n",
       "      <td>visite</td>\n",
       "      <td>visitar</td>\n",
       "      <td>vsp-1/3s</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345211</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862346</td>\n",
       "      <td>www.DineroAbundancia.com</td>\n",
       "      <td>www.dineroabundancia.com</td>\n",
       "      <td>n</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22924607 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SourceID     TokenID                      Word  \\\n",
       "0           431270  2206403096                      Este   \n",
       "1           431270  2206403097                        es   \n",
       "2           431270  2206403098                        un   \n",
       "3           431270  2206403099                      blog   \n",
       "4           431270  2206403100                        de   \n",
       "...            ...         ...                       ...   \n",
       "27345207    676060  2511862342                   obtener   \n",
       "27345208    676060  2511862343                       mas   \n",
       "27345209    676060  2511862344               informacion   \n",
       "27345210    676060  2511862345                    visite   \n",
       "27345211    676060  2511862346  www.DineroAbundancia.com   \n",
       "\n",
       "                             Lemma       POS Variety  \n",
       "0                             este       dd-      ES  \n",
       "1                              ser    vip-3s      ES  \n",
       "2                               un     li-ms      ES  \n",
       "3                             blog   nms          ES  \n",
       "4                               de         e      ES  \n",
       "...                            ...       ...     ...  \n",
       "27345207                   obtener        vr      ES  \n",
       "27345208                       mas        cc      ES  \n",
       "27345209               información         n      ES  \n",
       "27345210                   visitar  vsp-1/3s      ES  \n",
       "27345211  www.dineroabundancia.com         n      ES  \n",
       "\n",
       "[22924607 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df #Looks good, this removed about 1,500,000 more rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lastly, I only want to keep diminutivized forms for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = df[df['Word'].str.contains(r'\\w*i(t|ll)(o|a)s?\\b', regex=True)] #Keeps only rows ending in the segments of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403194</td>\n",
       "      <td>Nikita</td>\n",
       "      <td>nikita</td>\n",
       "      <td>o</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403206</td>\n",
       "      <td>escrito</td>\n",
       "      <td>escrito</td>\n",
       "      <td>jms</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>431290</td>\n",
       "      <td>2074527333</td>\n",
       "      <td>calladita</td>\n",
       "      <td>calladito</td>\n",
       "      <td>j</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>431290</td>\n",
       "      <td>2074527343</td>\n",
       "      <td>sólito</td>\n",
       "      <td>sólito</td>\n",
       "      <td>jms</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>431310</td>\n",
       "      <td>2143630275</td>\n",
       "      <td>necesita</td>\n",
       "      <td>necesitar</td>\n",
       "      <td>vip-3s</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27344546</td>\n",
       "      <td>676040</td>\n",
       "      <td>250005820</td>\n",
       "      <td>depósitos</td>\n",
       "      <td>depósito</td>\n",
       "      <td>nmp</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27344657</td>\n",
       "      <td>676040</td>\n",
       "      <td>250005931</td>\n",
       "      <td>precipita</td>\n",
       "      <td>precipitar</td>\n",
       "      <td>vip-3s</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27344674</td>\n",
       "      <td>676040</td>\n",
       "      <td>250005948</td>\n",
       "      <td>corralito</td>\n",
       "      <td>corralito</td>\n",
       "      <td>n</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27344789</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511861924</td>\n",
       "      <td>sencillos</td>\n",
       "      <td>sencillo</td>\n",
       "      <td>j</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27345089</td>\n",
       "      <td>676060</td>\n",
       "      <td>2511862224</td>\n",
       "      <td>permita</td>\n",
       "      <td>permitir</td>\n",
       "      <td>vsp-1/3s</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103923 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SourceID     TokenID       Word       Lemma       POS Variety\n",
       "98          431270  2206403194     Nikita      nikita         o      ES\n",
       "110         431270  2206403206    escrito     escrito   jms          ES\n",
       "321         431290  2074527333  calladita   calladito         j      ES\n",
       "331         431290  2074527343     sólito      sólito   jms          ES\n",
       "536         431310  2143630275   necesita   necesitar    vip-3s      ES\n",
       "...            ...         ...        ...         ...       ...     ...\n",
       "27344546    676040   250005820  depósitos    depósito   nmp          ES\n",
       "27344657    676040   250005931  precipita  precipitar    vip-3s      ES\n",
       "27344674    676040   250005948  corralito   corralito         n      ES\n",
       "27344789    676060  2511861924  sencillos    sencillo         j      ES\n",
       "27345089    676060  2511862224    permita    permitir  vsp-1/3s      ES\n",
       "\n",
       "[103923 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This gets the data to a first stage that's easier to work with. There are still many rows which do not belong in the data frame (e.g., verb forms other than gerunds that just happen to end in the same segment), but it will be more efficient to remove those when the full data frame is constructed. I'll delete unneeded objects before proceding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del fname\n",
    "del cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing corpus directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a first step, knowing now that the operations above are sucessful, I will define functions to make the processing pipeline for the full data drame object more efficient and streamlined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDF(fname):\n",
    "    \"\"\"Turns tab-delimited file into a data frame\"\"\"\n",
    "    cols = ['SourceID', 'TokenID', 'Word', 'Lemma', 'POS']\n",
    "    df = pd.read_csv(fname,sep='\\t',encoding ='iso-8859-1',skiprows=[0],header=None,names=cols)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_variety(df,variety):\n",
    "    \"\"\"Adds a column specifying a given variety of Spanish to a data frame\"\"\"\n",
    "    df['Variety'] = variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_syms(df):\n",
    "    \"\"\"Excludes symbols\"\"\"\n",
    "    df = df[df['POS'] != 'y']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redacted(df):\n",
    "    \"\"\"Excludes @ symbols, which stand for words that have been redacted for copyright reasons\"\"\"\n",
    "    df = df[df['Word'] != '@']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nondims(df):\n",
    "    \"\"\"Removes tokens that do not end in the segments of interest (-ito/-illo)\"\"\"\n",
    "    df = df[df['Word'].str.contains(r'\\w*i(t|ll)(o|a)s?\\b', regex=True)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's set the directory. I'll start with the Spain directory, since I used a Spain text file for the test run in Section 2 above. In addition, Spain is by far the largest directory (over 3 GB when compressed), so if the code works fine for it, it should work for the rest of the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../Diminutive-Suffix-Productivity/private/data/wlp_ES-sbo\\\\es-b-0.txt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dir = '../../Diminutive-Suffix-Productivity/private/data/'\n",
    "es_dir = glob.glob(corpus_dir + 'wlp_ES-sbo/*.txt')\n",
    "es_dir[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(es_dir) #There are 20 files in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SourceID, TokenID, Word, Lemma, POS]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_DF = pd.DataFrame(columns=['SourceID', 'TokenID', 'Word', 'Lemma', 'POS']) #Builds a new, empty data frame object.\n",
    "es_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in es_dir:                            #Note that I'm only using one of the cleaning functions\n",
    "    df = toDF(fname)                            #because it gets me to the same end result. But the other ones \n",
    "    df = remove_nondims(df)                     #might be useful down the road, which is why I defined them.\n",
    "    es_DF = pd.concat([es_DF, df], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>nikita</td>\n",
       "      <td>o</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403194</td>\n",
       "      <td>Nikita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>escrito</td>\n",
       "      <td>jms</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403206</td>\n",
       "      <td>escrito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>calladito</td>\n",
       "      <td>j</td>\n",
       "      <td>431290</td>\n",
       "      <td>2074527333</td>\n",
       "      <td>calladita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>sólito</td>\n",
       "      <td>jms</td>\n",
       "      <td>431290</td>\n",
       "      <td>2074527343</td>\n",
       "      <td>sólito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>necesitar</td>\n",
       "      <td>vip-3s</td>\n",
       "      <td>431310</td>\n",
       "      <td>2143630275</td>\n",
       "      <td>necesita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22146901</td>\n",
       "      <td>sencillo</td>\n",
       "      <td>jfs</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779969779</td>\n",
       "      <td>sencilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22147017</td>\n",
       "      <td>permitir</td>\n",
       "      <td>vsp-1/3s</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779969895</td>\n",
       "      <td>permita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22147166</td>\n",
       "      <td>inscrito</td>\n",
       "      <td>j</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779970044</td>\n",
       "      <td>inscrito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22147196</td>\n",
       "      <td>visita</td>\n",
       "      <td>nfp</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779970074</td>\n",
       "      <td>visitas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22147206</td>\n",
       "      <td>visita</td>\n",
       "      <td>nfp</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779970084</td>\n",
       "      <td>visitas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1872540 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Lemma       POS SourceID     TokenID       Word\n",
       "98           nikita         o   431270  2206403194     Nikita\n",
       "110         escrito   jms       431270  2206403206    escrito\n",
       "321       calladito         j   431290  2074527333  calladita\n",
       "331          sólito   jms       431290  2074527343     sólito\n",
       "536       necesitar    vip-3s   431310  2143630275   necesita\n",
       "...             ...       ...      ...         ...        ...\n",
       "22146901   sencillo   jfs      1891249  1779969779   sencilla\n",
       "22147017   permitir  vsp-1/3s  1891249  1779969895    permita\n",
       "22147166   inscrito         j  1891249  1779970044   inscrito\n",
       "22147196     visita   nfp      1891249  1779970074    visitas\n",
       "22147206     visita   nfp      1891249  1779970084    visitas\n",
       "\n",
       "[1872540 rows x 5 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_DF #It takes a while (which is expected because of the file sizes), but it works. I won't rerun the notebook\n",
    "      #because of the prior line but I've removed all extraneous cells for ease of reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_variety(es_DF,'ES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>SourceID</th>\n",
       "      <th>TokenID</th>\n",
       "      <th>Word</th>\n",
       "      <th>Variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>nikita</td>\n",
       "      <td>o</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403194</td>\n",
       "      <td>Nikita</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>escrito</td>\n",
       "      <td>jms</td>\n",
       "      <td>431270</td>\n",
       "      <td>2206403206</td>\n",
       "      <td>escrito</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>calladito</td>\n",
       "      <td>j</td>\n",
       "      <td>431290</td>\n",
       "      <td>2074527333</td>\n",
       "      <td>calladita</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>sólito</td>\n",
       "      <td>jms</td>\n",
       "      <td>431290</td>\n",
       "      <td>2074527343</td>\n",
       "      <td>sólito</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>necesitar</td>\n",
       "      <td>vip-3s</td>\n",
       "      <td>431310</td>\n",
       "      <td>2143630275</td>\n",
       "      <td>necesita</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22146901</td>\n",
       "      <td>sencillo</td>\n",
       "      <td>jfs</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779969779</td>\n",
       "      <td>sencilla</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22147017</td>\n",
       "      <td>permitir</td>\n",
       "      <td>vsp-1/3s</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779969895</td>\n",
       "      <td>permita</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22147166</td>\n",
       "      <td>inscrito</td>\n",
       "      <td>j</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779970044</td>\n",
       "      <td>inscrito</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22147196</td>\n",
       "      <td>visita</td>\n",
       "      <td>nfp</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779970074</td>\n",
       "      <td>visitas</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22147206</td>\n",
       "      <td>visita</td>\n",
       "      <td>nfp</td>\n",
       "      <td>1891249</td>\n",
       "      <td>1779970084</td>\n",
       "      <td>visitas</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1872540 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Lemma       POS SourceID     TokenID       Word Variety\n",
       "98           nikita         o   431270  2206403194     Nikita      ES\n",
       "110         escrito   jms       431270  2206403206    escrito      ES\n",
       "321       calladito         j   431290  2074527333  calladita      ES\n",
       "331          sólito   jms       431290  2074527343     sólito      ES\n",
       "536       necesitar    vip-3s   431310  2143630275   necesita      ES\n",
       "...             ...       ...      ...         ...        ...     ...\n",
       "22146901   sencillo   jfs      1891249  1779969779   sencilla      ES\n",
       "22147017   permitir  vsp-1/3s  1891249  1779969895    permita      ES\n",
       "22147166   inscrito         j  1891249  1779970044   inscrito      ES\n",
       "22147196     visita   nfp      1891249  1779970074    visitas      ES\n",
       "22147206     visita   nfp      1891249  1779970084    visitas      ES\n",
       "\n",
       "[1872540 rows x 6 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_DF  #It appears the order of the columns was shuffled for some reason. I will fix this once I have constructed \n",
    "       #the final data frame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spain is ready, now I have to do the same for the remaning 20 countries. This will take me some time, but I don't expect the code above to run into issues as the files are all in the same format. Below are the corpus directories for the remaining countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_dir = glob.glob(corpus_dir + 'wlp_AR-tez/*.txt') #Argentina\n",
    "bo_dir = glob.glob(corpus_dir + 'wlp_BO-teh/*.txt') #Bolivia\n",
    "cl_dir = glob.glob(corpus_dir + 'wlp_CL-wts/*.txt') #Chile\n",
    "co_dir = glob.glob(corpus_dir + 'wlp_CO-pem/*.txt') #Colombia\n",
    "cr_dir = glob.glob(corpus_dir + 'wlp_CR-jfh/*.txt') #Costa Rica\n",
    "cu_dir = glob.glob(corpus_dir + 'wlp_CU-rag/*.txt') #Cuba\n",
    "do_dir = glob.glob(corpus_dir + 'wlp_DO-egn/*.txt') #Dominican Republic\n",
    "ec_dir = glob.glob(corpus_dir + 'wlp_EC-jss/*.txt') #Ecuador\n",
    "gt_dir = glob.glob(corpus_dir + 'wlp_GT-miv/*.txt') #Guatemala\n",
    "hn_dir = glob.glob(corpus_dir + 'wlp_HN-paj/*.txt') #Honduras\n",
    "mx_dir = glob.glob(corpus_dir + 'wlp_MX-vzo/*.txt') #Mexico\n",
    "ni_dir = glob.glob(corpus_dir + 'wlp_NI-exu/*.txt') #Nicaragua\n",
    "pa_dir = glob.glob(corpus_dir + 'wlp_PA-qlz/*.txt') #Panama\n",
    "pe_dir = glob.glob(corpus_dir + 'wlp_PE-tae/*.txt') #Peru\n",
    "pr_dir = glob.glob(corpus_dir + 'wlp_PR-epz/*.txt') #Puerto Rico\n",
    "py_dir = glob.glob(corpus_dir + 'wlp_PY-ukd/*.txt') #Paraguay\n",
    "sv_dir = glob.glob(corpus_dir + 'wlp_SV-xkl/*.txt') #El Salvador\n",
    "us_dir = glob.glob(corpus_dir + 'wlp_US-ufh/*.txt') #The US\n",
    "uy_dir = glob.glob(corpus_dir + 'wlp_UY-nde/*.txt') #Uruguay\n",
    "#ve_dir = glob.glob(corpus_dir + 'wlp_VE-wsc/*.txt') #Venezuela. This file (ironically) doesn't work. I'll have to \n",
    "                                                       #go to the LMC and try to get a new copy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On to building data frames. For this purpose, I have created a master function that goes through all of the steps above. I'll run each country in one cell so that memory and time limitations won't be exceeded (hency why data frames are deleted after being save). Likewise, so that I don't have to rerun the whole notebook each time (or in case I want to use the resulting data frames in a different notebook), it makes sense to save the results in their current state. For this I'll make use of Pandas' own pickling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_process(fdir, country_df, variety):\n",
    "    \"\"\"Builds, cleans, and creates a data frame using all files from the corpus directory and\n",
    "    keeping only rows of interest. fdir corresponds to the directory of the country, country_df\n",
    "    is a string and corresponds to an empty data frame to be populated, variety is also a string\n",
    "    and correspond to the variety being processed.\"\"\"\n",
    "    country_df = pd.DataFrame(columns=['SourceID', 'TokenID', 'Word', 'Lemma', 'POS'])\n",
    "    for fname in fdir:                         \n",
    "        df = toDF(fname)                           \n",
    "        df = remove_nondims(df)\n",
    "        country_df = pd.concat([country_df, df], sort=True)\n",
    "    add_variety(country_df, variety)\n",
    "    return country_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argentina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_DF = corpus_process(ar_dir, 'ar_DF', 'AR')\n",
    "ar_DF.to_pickle('ar_DF.pkl')\n",
    "del ar_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bolivia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_DF = corpus_process(bo_dir, 'bo_DF', 'BO')\n",
    "bo_DF.to_pickle('bo_DF.pkl')\n",
    "del bo_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_DF = corpus_process(cl_dir, 'cl_DF', 'CL')\n",
    "cl_DF.to_pickle('cl_DF.pkl')\n",
    "del cl_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colombia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_DF = corpus_process(co_dir, 'co_DF', 'CO')\n",
    "co_DF.to_pickle('co_DF.pkl')\n",
    "del co_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costa Rica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_DF = corpus_process(cr_dir, 'cr_DF', 'CR')\n",
    "cr_DF.to_pickle('cr_DF.pkl')\n",
    "del cr_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_DF = corpus_process(cu_dir, 'cu_DF', 'CU')\n",
    "cu_DF.to_pickle('cu_DF.pkl')\n",
    "del cu_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dominican Republic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_DF = corpus_process(do_dir, 'do_DF', 'DO')\n",
    "do_DF.to_pickle('do_DF.pkl')\n",
    "del do_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecuador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_DF = corpus_process(ec_dir, 'ec_DF', 'EC')\n",
    "ec_DF.to_pickle('ec_DF.pkl')\n",
    "del ec_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guatemala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_DF = corpus_process(gt_dir, 'gt_DF', 'GT')\n",
    "gt_DF.to_pickle('gt_DF.pkl')\n",
    "del gt_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honduras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_DF = corpus_process(hn_dir, 'hn_DF', 'HN')\n",
    "hn_DF.to_pickle('hn_DF.pkl')\n",
    "del hn_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mexico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_DF = corpus_process(mx_dir, 'mx_DF', 'MX')\n",
    "mx_DF.to_pickle('mx_DF.pkl')\n",
    "del mx_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicaragua:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni_DF = corpus_process(ni_dir, 'ni_DF', 'NI')\n",
    "ni_DF.to_pickle('ni_DF.pkl')\n",
    "del ni_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_DF = corpus_process(pa_dir, 'pa_DF', 'PA')\n",
    "pa_DF.to_pickle('pa_DF.pkl')\n",
    "del pa_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_DF = corpus_process(pe_dir, 'pe_DF', 'PE')\n",
    "pe_DF.to_pickle('pe_DF.pkl')\n",
    "del pe_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puerto Rico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_DF = corpus_process(pr_dir, 'pr_DF', 'PR')\n",
    "pr_DF.to_pickle('pr_DF.pkl')\n",
    "del pr_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paraguay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_DF = corpus_process(py_dir, 'py_DF', 'PY')\n",
    "py_DF.to_pickle('py_DF.pkl')\n",
    "del py_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Salvador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_DF = corpus_process(sv_dir, 'sv_DF', 'SV')\n",
    "sv_DF.to_pickle('sv_DF.pkl')\n",
    "del sv_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_DF = corpus_process(us_dir, 'us_DF', 'US')\n",
    "us_DF.to_pickle('us_DF.pkl')\n",
    "del us_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uruguay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "uy_DF = corpus_process(uy_dir, 'uy_DF', 'UY')\n",
    "uy_DF.to_pickle('uy_DF.pkl')\n",
    "del uy_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Venezuela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ve_DF = corpus_process(ve_dir, 've_DF', 'VE')\n",
    "#ve_DF.to_pickle('ve_DF.pkl')\n",
    "#del ve_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- That's the end of this stage. I now have a preliminary data frame for each country that I can later put together into a larger one. I still want to keep the individual country data frames, though, for by-country analysis and processing or in case I notice an issue down the road. As a last step, I will need to know the number of hapax legomena in each subset of the corpus, so I'll extract it as a set also using a master function that incorporates some of the other functions I have defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hapax(fdir, country_hapax):\n",
    "    \"\"\"Creates a set of hapax legomena. fdir corresponds to the directory of the country, country_hapax \n",
    "    is a string and corresponds to an empty data frame to be populated.\"\"\"\n",
    "    country_hapax = set()\n",
    "    for fname in fdir:                         \n",
    "        df = toDF(fname)    \n",
    "        df = remove_syms(df)\n",
    "        df = remove_redacted(df)\n",
    "        hapax = set([w.lower() for w in df['Word']])\n",
    "        for word in hapax:\n",
    "            country_hapax.add(word)\n",
    "    return country_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Argentina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-02619b4e1763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mar_hapax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_hapax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ar_hapax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ar_hapax.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar_hapax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mar_hapax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-ab35d341edbc>\u001b[0m in \u001b[0;36mextract_hapax\u001b[1;34m(fdir, country_hapax)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfdir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_syms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_redacted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mhapax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-aae2e7fafea0>\u001b[0m in \u001b[0;36mremove_syms\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_syms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Excludes symbols\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'POS'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2969\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2973\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3023\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3024\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3025\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3027\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3603\u001b[0m         new_data = self._data.take(\n\u001b[1;32m-> 3604\u001b[1;33m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3605\u001b[0m         )\n\u001b[0;32m   3606\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1393\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Indices must be nonzero and less than the axis length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1395\u001b[1;33m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1396\u001b[0m         return self.reindex_indexer(\n\u001b[0;32m   1397\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[0;32m    900\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Unable to fill values because {0} cannot contain NA\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m             \u001b[0mtaken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    903\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shallow_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ar_hapax = extract_hapax(ar_dir, 'ar_hapax')\n",
    "with open('ar_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(ar_hapax, f, -1)\n",
    "del ar_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bolivia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_hapax = extract_hapax(bo_dir, 'bo_hapax')\n",
    "with open('bo_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(bo_hapax, f, -1)\n",
    "del bo_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_hapax = extract_hapax(cl_dir, 'cl_hapax')\n",
    "with open('cl_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(cl_hapax, f, -1)\n",
    "del cl_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Colombia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_hapax = extract_hapax(co_dir, 'co_hapax')\n",
    "with open('co_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(co_hapax, f, -1)\n",
    "del co_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Costa Rica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_hapax = extract_hapax(cr_dir, 'cr_hapax')\n",
    "with open('cr_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(cr_hapax, f, -1)\n",
    "del cr_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cuba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_hapax = extract_hapax(cu_dir, 'cu_hapax')\n",
    "with open('cu_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(cu_hapax, f, -1)\n",
    "del cu_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dominican Republic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_hapax = extract_hapax(do_dir, 'do_hapax')\n",
    "with open('do_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(do_hapax, f, -1)\n",
    "del do_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ecuador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_hapax = extract_hapax(ec_dir, 'ec_hapax')\n",
    "with open('ec_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(ec_hapax, f, -1)\n",
    "del ec_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_hapax = extract_hapax(es_dir, 'es_hapax')\n",
    "with open('es_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(es_hapax, f, -1)\n",
    "del es_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guatamela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_hapax = extract_hapax(gt_dir, 'gt_hapax')\n",
    "with open('gt_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(gt_hapax, f, -1)\n",
    "del gt_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Honduras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_hapax = extract_hapax(hn_dir, 'hn_hapax')\n",
    "with open('hn_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(hn_hapax, f, -1)\n",
    "del hn_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mexico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_hapax = extract_hapax(mx_dir, 'mx_hapax')\n",
    "with open('mx_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(mx_hapax, f, -1)\n",
    "del mx_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nicaragua:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni_hapax = extract_hapax(ni_dir, 'ni_hapax')\n",
    "with open('ni_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(ni_hapax, f, -1)\n",
    "del ni_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Panama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_hapax = extract_hapax(pa_dir, 'pa_hapax')\n",
    "with open('pa_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(pa_hapax, f, -1)\n",
    "del pa_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Peru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_hapax = extract_hapax(pe_dir, 'pe_hapax')\n",
    "with open('pe_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(pe_hapax, f, -1)\n",
    "del pe_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Puerto Rico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_hapax = extract_hapax(pr_dir, 'pr_hapax')\n",
    "with open('pr_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(pr_hapax, f, -1)\n",
    "del pr_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paraguay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_hapax = extract_hapax(py_dir, 'py_hapax')\n",
    "with open('py_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(py_hapax, f, -1)\n",
    "del py_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El Salvador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_hapax = extract_hapax(sv_dir, 'bsvo_hapax')\n",
    "with open('sv_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(sv_hapax, f, -1)\n",
    "del sv_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The US:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_hapax = extract_hapax(us_dir, 'us_hapax')\n",
    "with open('us_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(us_hapax, f, -1)\n",
    "del us_hapax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uruguay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uy_hapax = extract_hapax(uy_dir, 'uy_hapax')\n",
    "with open('uy_hapax.pkl', 'wb') as f:\n",
    "    pickle.dump(uy_hapax, f, -1)\n",
    "del uy_hapax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
